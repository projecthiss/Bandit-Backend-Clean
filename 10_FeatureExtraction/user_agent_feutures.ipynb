{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8wYEImfRfLg",
        "outputId": "7baf33d7-b7df-439d-f767-e19c4c3f54f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install\n",
        "!pip install Pyphen\n",
        "!pip install langdetect\n",
        "!pip install pyspellchecker\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V_3qTV4RvLk",
        "outputId": "58ccf6f9-1802-48e0-fa57-7d9d37e23cda"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Pyphen in /usr/local/lib/python3.8/dist-packages (0.13.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.8/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from langdetect) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.8/dist-packages (0.7.1)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "2023-01-18 16:41:58.817594: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-lg==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (21.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.11)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.10.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading libraries ...\")\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import pyphen\n",
        "from datetime import datetime\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import Tree\n",
        "from textblob import TextBlob\n",
        "from langdetect import detect\n",
        "from langdetect import detect_langs\n",
        "from spellchecker import SpellChecker\n",
        "import collections as ct\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "print('loaded')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSrVRf2sR1lj",
        "outputId": "e1d38250-b7a2-4e80-8e0e-5355d078483a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading libraries ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "spell = SpellChecker()\n",
        "stopwords = nlp.Defaults.stop_words\n",
        "dic = pyphen.Pyphen(lang='en')\n",
        "sentiment = SentimentIntensityAnalyzer()\n",
        "p_stemmer = PorterStemmer()\n",
        "tfidf_vectorizer=TfidfVectorizer(use_idf=True, stop_words=stopwords) \n",
        "print('Initialization done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYh5R1AiTkwL",
        "outputId": "df4eac20-3bf6-429c-e3dc-614a3d832c10"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialization done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/drive/MyDrive/output_user_agent_no_names.xlsx', sheet_name=0)\n",
        "print(\"Succesfully read excel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxYNn1lewkm6",
        "outputId": "7af18374-3a6f-489e-923d-a8a10e4267d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Succesfully read excel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Scoring:\n",
        "    def __init__(self, ticket='', unclean_ticket=''):\n",
        "        self.unclean_ticket = unclean_ticket\n",
        "        self.ticket = ticket\n",
        "        if not isinstance(self.ticket, str):\n",
        "            self.ticket = ''\n",
        "        # self.doc = nlp(self.ticket)\n",
        "        self.alph = re.sub(r'[^a-zA-Z ]', '', self.ticket)\n",
        "        self.words = list(filter(None, self.alph.split(' ')))\n",
        "        # self.entities_sw = [i for i in self.doc.ents if i.label_ not in ['ORG', 'WORK_OF_ART', 'PRODUCT']]\n",
        "        self.stopwords = stopwords\n",
        "\n",
        "    def get_sentences(self):\n",
        "        sentences = [sent.text.strip() for sent in self.doc.sents]\n",
        "        for i, sent in enumerate(sentences):\n",
        "            sent = re.sub(r'[^a-zA-Z]', '', sent)\n",
        "            if sent == '':\n",
        "                sentences[i] = ''\n",
        "        sentences = list(filter(None, sentences))\n",
        "        if len(sentences) == 0:\n",
        "            return None, 1\n",
        "        return sentences, len(sentences)\n",
        "\n",
        "    def get_words(self):\n",
        "        return len(self.words)\n",
        "\n",
        "    def words_per_sentences(self):\n",
        "        if self.get_sentences()[1] > 0:\n",
        "            return float(\"{:.2f}\".format(self.get_words() / self.get_sentences()[1]))\n",
        "        else:\n",
        "            return self.get_words()\n",
        "\n",
        "    def no_stopwords(self):\n",
        "        sw_count = 0\n",
        "        for word in self.words:\n",
        "            if word.lower() in self.stopwords:\n",
        "                sw_count += 1\n",
        "        # sw_count += len(self.entities_sw)\n",
        "        no_sw = self.get_words() - sw_count\n",
        "        return sw_count, self.get_words() - sw_count, float(\"{:.2f}\".format(no_sw / self.get_sentences()[1]))\n",
        "\n",
        "    def get_nouns(self):\n",
        "        n_count = 0\n",
        "        for noun in self.doc.noun_chunks:\n",
        "            if str(noun).lower() not in self.stopwords:\n",
        "                n_count += 1\n",
        "        return n_count\n",
        "\n",
        "    def get_pronouns(self):\n",
        "        pn_count = 0\n",
        "        for pn in self.doc:\n",
        "            if pn.tag_[0] == 'P':\n",
        "                pn_count += 1\n",
        "        return pn_count\n",
        "\n",
        "    def get_readablity(self):\n",
        "        suffix = [\"ing\", \"ed\", \"es\", \"ous\", \"tion\", \"nce\", \"ness\"]\n",
        "        complex_words = []\n",
        "        new_ticket = self.ticket.translate(str.maketrans('', '', string.punctuation))\n",
        "        w = float(self.get_words())\n",
        "        s = float(self.get_sentences()[1])\n",
        "        # for ent in self.entities_sw:\n",
        "        #     new_ticket = new_ticket.replace(str(ent), '')\n",
        "        new_ticket = new_ticket.replace('-', '')\n",
        "        syllables_count = 0\n",
        "        for wrd in new_ticket.split(' '):\n",
        "            ws = dic.inserted(wrd).count('-')\n",
        "            syllables_count += ws + 1\n",
        "            for suf in suffix:\n",
        "                if wrd.endswith(suf):\n",
        "                    ws -= 1\n",
        "            if ws >= 2:\n",
        "                complex_words.append(wrd)\n",
        "        # words_count_nt = self.get_words() - len(self.entities_sw)\n",
        "        # Flesch\n",
        "        syllables_count = float(syllables_count)\n",
        "        if s == 0 or w == 0:\n",
        "          score1 = 0\n",
        "          score2 = 0\n",
        "          score3 = 0\n",
        "        else:\n",
        "          score1 = 206.835 - 1.015 * w / s - 84.6 * syllables_count / w\n",
        "          # Coleman–Liau\n",
        "          letters = float(len(new_ticket) - new_ticket.count(' '))\n",
        "          score2 = 0.0588 * letters / w * 100 - 0.296 * s / w * 100 - 15.8\n",
        "          # Gunning fog\n",
        "          score3 = 0.4 * (w / s + 100 * float(len(complex_words)) / w)\n",
        "        return float(\"{:.2f}\".format(score1)), float(\"{:.2f}\".format(score2)), float(\"{:.2f}\".format(score3))\n",
        "\n",
        "    def get_verbs(self):\n",
        "        v_count = 0\n",
        "        for v in self.doc:\n",
        "            if v.tag_[0] == 'V':\n",
        "                v_count += 1\n",
        "        return v_count\n",
        "\n",
        "    @staticmethod\n",
        "    def get_links(ticket_original):\n",
        "        links = re.findall('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+', ticket_original)\n",
        "        links = filter(lambda x: x[:5] != 'image', links)\n",
        "        flinks = list(links)\n",
        "        return flinks, len(flinks)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_attachments(ticket_original):\n",
        "        c = 0\n",
        "        cc = 0\n",
        "        if '[' in ticket_original and ']' in ticket_original:\n",
        "            new_s = ticket_original\n",
        "            while '[' in new_s:\n",
        "                ia = new_s.find(\"[\")\n",
        "                text = new_s[new_s.find(\"[\") + 1:new_s.find(\"]\", ia)]\n",
        "                if text != '[' and text != ']' and len(text) > 0:\n",
        "                    if text == 'code':\n",
        "                        cc += 1\n",
        "                    else:\n",
        "                        c += 1\n",
        "                new_s = new_s[:ia] + new_s[ia + 1:]\n",
        "        return int(cc / 2) + c\n",
        "\n",
        "    @staticmethod\n",
        "    def get_interacts(ticket_original):\n",
        "        matches = re.findall(r'\\d{4}-\\d{2}-\\d{2} \\d{2}\\d{2}\\d{2}', ticket_original)\n",
        "        return len(matches)\n",
        "\n",
        "    def preprocess_lem(self):\n",
        "        alph_ticket = self.alph\n",
        "        lemdoc = nlp(alph_ticket)\n",
        "        lemticket = ' '.join([token.lemma_ for token in lemdoc])\n",
        "        return lemticket\n",
        "\n",
        "    def get_sentiment(self):\n",
        "        ticket = Scoring(self.ticket).preprocess_lem()\n",
        "        sentiment_score = TextBlob(ticket).sentiment.polarity\n",
        "        if sentiment_score < 0:\n",
        "            sentiment = 'Negative'\n",
        "        elif sentiment_score == 0:\n",
        "            sentiment = 'Neutral'\n",
        "        else:\n",
        "            sentiment = 'Positive'\n",
        "\n",
        "        return sentiment_score, sentiment\n",
        "\n",
        "    def get_refrences(self):\n",
        "        ticket_low = self.ticket.lower()\n",
        "        return ticket_low.count('called') + ticket_low.count('call') + ticket_low.count('sent') + ticket_low.count(\n",
        "            'send') + ticket_low.count('messaged')\n",
        "\n",
        "    @staticmethod\n",
        "    def get_datediff(date1, date2):\n",
        "        date_format = \"%Y-%m-%d\"\n",
        "        a = datetime.strptime(date1, date_format)\n",
        "        b = datetime.strptime(date2, date_format)\n",
        "        c = datetime.now()\n",
        "        delta = b - a\n",
        "        delta2 = c - a\n",
        "        return delta.days, delta2.days\n",
        "\n",
        "    def get_language_confidence(self):\n",
        "        ticket = Scoring(self.ticket).preprocess_lem()\n",
        "        confidence = str(detect_langs(ticket)[0]).partition(':')[2]\n",
        "        confidence = float(confidence)\n",
        "\n",
        "        return confidence\n",
        "\n",
        "    def get_special_characters(self):\n",
        "        special_chars = ['#', '+', '~', '/', '[', ']', '<', '>', '{', '}']\n",
        "        return sum(v for k, v in ct.Counter(self.unclean_ticket).items() if k in special_chars)\n",
        "\n",
        "    def get_bulleted_list(self):\n",
        "        bullet_style = ['#', '*', '++', '-']\n",
        "        count_bullet_style = 0\n",
        "        for style in bullet_style:\n",
        "            count = self.unclean_ticket.count(style)\n",
        "            if count < 7 and count >= 3:\n",
        "                count_bullet_style += 1\n",
        "\n",
        "        check = False\n",
        "        if count_bullet_style > 0:\n",
        "            check = True\n",
        "\n",
        "        return check, count_bullet_style\n",
        "      \n",
        "    def get_spelling_mistakes(self):\n",
        "        ticket = re.findall(\"[a-zA-Z,.]+\", self.ticket)\n",
        "        words = re.findall(\"[a-zA-Z,]+\", self.ticket)\n",
        "        misspelled = spell.unknown(ticket)\n",
        "\n",
        "        return len(misspelled) / len(words)\n",
        "\n",
        "    def get_verb_noun_combinations(self):\n",
        "        nosw_ticket = ' '.join([w for w in self.ticket.split(' ') if w not in self.stopwords])\n",
        "        pos_doc1 = nlp(nosw_ticket)\n",
        "        pos_doc2 = self.doc\n",
        "        pos_list1 = \"\"\n",
        "        pos_list2 = \"\"\n",
        "        for token in pos_doc1:\n",
        "            pos_list1 += ' ' + (token.pos_)\n",
        "        for token in pos_doc2:\n",
        "            pos_list2 += ' ' + (token.pos_)\n",
        "        return pos_list1.count('VERB NOUN'), pos_list2.count('VERB NOUN')\n",
        "\n",
        "    def to_nltk_tree(self, node):\n",
        "        if node.n_lefts + node.n_rights > 0:\n",
        "            return Tree(node.orth_, [self.to_nltk_tree(child) for child in node.children])\n",
        "        else:\n",
        "            return node.orth_\n",
        "\n",
        "    def get_max_depth(self):\n",
        "        nlp_text = nlp(self.ticket)\n",
        "        max_depth = 0\n",
        "        for sentence in nlp_text.sents:\n",
        "            tree = self.to_nltk_tree(sentence.root)\n",
        "            if not isinstance(tree, str):\n",
        "                    max_depth = max(max_depth, tree.height())\n",
        "        return max_depth\n",
        "\n",
        "    def get_specific_words_count(self):\n",
        "        text = [word for word in self.words if len(word) > 3]\n",
        "        stemmed_tokens = [p_stemmer.stem(i) for i in text]\n",
        "        en_filtered = [w for w in stemmed_tokens if not w in self.stopwords]\n",
        "        stemmed_tokens = [p_stemmer.stem(i) for i in en_filtered]\n",
        "        return len(stemmed_tokens)\n",
        "\n",
        "    def get_unknown_words_count(self):\n",
        "        ticket = self.ticket\n",
        "        with open('/content/drive/MyDrive/unknown_words.txt') as f:\n",
        "            lines = f.readlines()\n",
        "        unknown_words = [x.replace('\\n', '') for x in lines]\n",
        "        new_unknown_words = []\n",
        "        for word in unknown_words:\n",
        "            new_unknown_words.append(word.split())\n",
        "        flattened  = [val for sublist in new_unknown_words for val in sublist]\n",
        "        final_unknown_words = [x.replace(',', '') for x in flattened]\n",
        "        count_unknown_words = 0\n",
        "        for unknown_word in final_unknown_words:\n",
        "            count_unknown_words += ticket.count(' ' + unknown_word + ' ')\n",
        "        return count_unknown_words\n",
        "\n",
        "    def get_inquery_intensity(self): \n",
        "        return self.ticket.count('?')"
      ],
      "metadata": {
        "id": "cp92QHSETizg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_lines_with_date_start(text):\n",
        "    try:\n",
        "        splitted_text = text.split(\"\\n\")\n",
        "    except:\n",
        "        splitted_text = []\n",
        "    new_text = \"\"\n",
        "    for i, sp_text in enumerate(splitted_text):\n",
        "        if not re.match(\"[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}[0-9]{2}[0-9]{2}\", sp_text[0:19]):\n",
        "            new_text += sp_text + \"\\n\"\n",
        "        else:\n",
        "            if not re.match(\"[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}[0-9]{2}[0-9]{2}\", splitted_text[i - 1]):\n",
        "                new_text += \"______________________________\" + \"\\n\"\n",
        "            else:\n",
        "                new_text += \"\\n\"\n",
        "    return new_text.rstrip()\n",
        "\n",
        "\n",
        "def remove_lines_start(text, start):\n",
        "    greetings = [\"good day\", \"hi\", \"hello\", \"dear\", \"many thanks\", \"thank you\"]\n",
        "    try:\n",
        "        splitted_text = text.split(\"\\n\")\n",
        "    except:\n",
        "        splitted_text = []\n",
        "    new_text = \"\"\n",
        "    for sp_text in splitted_text:\n",
        "        cond = True\n",
        "        if not sp_text[0:len(start)] == start:\n",
        "            clean_sp_text = ''.join([d for d in sp_text if not d.isdigit()])\n",
        "            clean_sp_text = clean_sp_text.replace('.', '')\n",
        "            clean_sp_text = clean_sp_text.replace('-', '')\n",
        "            clean_sp_text_list = list(filter(None, clean_sp_text.strip().split(\" \")))\n",
        "            # print(clean_sp_text_list)\n",
        "            if len(clean_sp_text_list) <= 5:\n",
        "                for greeting in greetings:\n",
        "                    if clean_sp_text.lower().strip().startswith(greeting):\n",
        "                        cond = False\n",
        "                if len(clean_sp_text_list) <= 1:\n",
        "                    cond = False\n",
        "\n",
        "            if sp_text == \"______________________________\":\n",
        "                cond = True\n",
        "            if cond:\n",
        "                new_text += sp_text + \"\\n\"\n",
        "\n",
        "    return new_text.rstrip()\n",
        "\n",
        "\n",
        "def preprocess(ticket):\n",
        "    ticket = '\\n' + str(ticket)\n",
        "\n",
        "    # remove names after @\n",
        "    for i, w in enumerate(ticket):\n",
        "        if w == '@':\n",
        "            y = ticket.find(' ', i)\n",
        "            z = ticket.find(' ', y+1)\n",
        "            ticket = ticket.replace(ticket[i:z], '')\n",
        "\n",
        "    index = [m.start() for m in re.finditer(re.escape('code]'), ticket)]\n",
        "    if len(index) > 0:\n",
        "        for i, ind in enumerate(index):\n",
        "            try:\n",
        "                index = [m.start() for m in re.finditer(re.escape('code]'), ticket)]\n",
        "                start_index = index[0] - 1\n",
        "                end_index = index[1] + 5\n",
        "                ticket = ticket[:start_index] + ticket[end_index + 1:]\n",
        "                index = [m.start() for m in re.finditer(re.escape('code]'), ticket)]\n",
        "                if len(index) < 2:\n",
        "                    break\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    extras = ['[', ']', 'cid', '+', '(', ')', ':', \"xD\",\n",
        "              '<', '>', '#', '~', '/', '{', '}', \"Issue description\", \"_x000D_\", ';', ',', \"Pvt  Limited\", \"Short Description\"\n",
        "              , \"My action taken\", \"Issues\", \"Action taken\", \"Directory Services Administrator\", \"PM\", \"AM\", \"nan\"]\n",
        "\n",
        "    for extra in extras:\n",
        "        ticket = ticket.replace(extra, '')\n",
        "\n",
        "    text = \"The information contained in this communication\"\n",
        "    if text in ticket:\n",
        "        extra_text_index = ticket.find(\"The information contained in this communication\")\n",
        "        extra_lines = ticket[extra_text_index:].split('\\n')[:9]\n",
        "        for line in extra_lines:\n",
        "            ticket = ticket.replace(line, '')\n",
        "\n",
        "    ticket = re.sub('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+', '', ticket)\n",
        "\n",
        "    ticket = '\\n'.join(line.strip() for line in ticket.split('\\n') if line != '' and \"khayaban\" not in line.lower())\n",
        "    line_beginnings_remove = [\"Additional Location Information\", \"Affected Device\", \"received from\", \"Email\",\n",
        "                              \"Tel\", \"Subject\", \"From\", \"Sent\", \"To\", \"reply from\", \"Issue(s)\"\n",
        "                              , \"Who called\", \"Ext Number\", \"Locations\", \"Device\", \"Call time\", \"Computer\", \"Client Type\", \"Version VPN\", \"PC Name\", \"Keyword\",\n",
        "                              \"Sent\", \"From\"]\n",
        "\n",
        "\n",
        "    for beginning in line_beginnings_remove:\n",
        "        ticket = remove_lines_start(ticket, beginning)\n",
        "\n",
        "    ticket_with_time = ticket\n",
        "    ticket = remove_lines_with_date_start(ticket)\n",
        "    doc = nlp(ticket)\n",
        "    names = [i for i in doc.ents if i.label_.lower() in [\"person\", \"gpe\", \"location\"]]\n",
        "    for name in names:\n",
        "        ticket = ticket.replace(str(name), '')\n",
        "    ticket = '\\n '.join(line.strip() for i, line in enumerate(ticket.split('\\n')) if line != '' and (1 < i < len(ticket.split('\\n')) - 1 or line != \"______________________________\"))\n",
        "    return ticket.strip(), ticket_with_time.strip()"
      ],
      "metadata": {
        "id": "85TQDku5VEuD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24L2u998UlNN",
        "outputId": "b39dccf1-2ea0-4a9a-cfbb-8dc79c4b30f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'Unnamed: 0.1', 'number', 'opened_by',\n",
              "       'short_description', 'priority', 'assignment_group', 'assigned_to',\n",
              "       'u_main_category_reporting', 'u_subcategory_1_reporting',\n",
              "       'u_subcategory_2_reporting', 'u_subcategory_3_reporting',\n",
              "       'u_resolver_group', 'u_resolver', 'u_first_assignment_group',\n",
              "       'u_country', 'sys_updated_on', 'sys_updated_by', 'sys_mod_count',\n",
              "       'u_vip', 'u_type', 'u_region', 'contact_type', 'location', 'u_resolved',\n",
              "       'closed_at', 'u_assignment_group_history', 'work_notes', 'comments',\n",
              "       'close_notes', 'u_assignee_history', 'merged', 'merged_string',\n",
              "       'business_duration', 'u_reassignment_count_assigne',\n",
              "       'reassignment_count', 'cleaned_string', 'isEnglish', 'user_comment',\n",
              "       'agent_comment', 'problem', 'solution', 'solution_sorted'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = df.loc[10, \"problem\"]"
      ],
      "metadata": {
        "id": "AyQdbMHyUfba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "qVA-X3DaWMKy",
        "outputId": "c93b8647-b7ce-4d13-95e6-6838e4aae821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Please reset the CITRIX password and inform new password through user's mobile number.\\n2021-01-01 13:06:07 - [name_5] (Additional comments)\\nAdditional Location Information: \\nAffected Device: \\nIssue description: Please reset the CITRIX password and inform new password through user's mobile number.\\nContact details: +91[phoneNumber_1819]011\\n\\n\\nnan\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text = []\n",
        "preprocessed_text_time = []\n",
        "sentences_count = []\n",
        "words_count = []\n",
        "words_per_sentence = []\n",
        "stop_words_count = []\n",
        "words_count_no_stop_words = []\n",
        "words_per_sentence_no_stop_words = []\n",
        "nouns_count = []\n",
        "verbs_count = []\n",
        "pronoun_count = []\n",
        "flesch = []\n",
        "coleman = []\n",
        "gunning = []\n",
        "open_close = []\n",
        "open_now = []\n",
        "sentiment_score = []\n",
        "sentiment = []\n",
        "language_confidence = []\n",
        "inquery_intensity = []\n",
        "special_characters = []\n",
        "bulleted_list = []\n",
        "bulleted_list_count = []\n",
        "spelling_mistakes = []\n",
        "verb_noun_combinations = []\n",
        "pos_tagging_max_depth = []\n",
        "specific_word_count = []\n",
        "# unknown_words_count = []\n",
        "refs = []\n",
        "links_count = []\n",
        "attachments_count = []\n",
        "interactions_count = []\n",
        "\n",
        "\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    ticket = str(row['problem'])\n",
        "    ticket_cleantxt, ticket_with_time = preprocess(ticket)\n",
        "    preprocessed_text.append(ticket_cleantxt)\n",
        "    preprocessed_text_time.append(ticket_with_time)\n",
        "    my_ticket = Scoring(ticket_cleantxt.strip(), ticket.strip())\n",
        "    ref = my_ticket.get_refrences()\n",
        "    refs.append(ref)\n",
        "    s1, s2 = my_ticket.get_sentences()\n",
        "    sentences_count.append(s2)\n",
        "    words_count.append(my_ticket.get_words())\n",
        "    words_per_sentence.append(my_ticket.words_per_sentences())\n",
        "    nsp1, nsp2, nsp3 = my_ticket.no_stopwords()\n",
        "    stop_words_count.append(nsp1)\n",
        "    words_count_no_stop_words.append(nsp2)\n",
        "    words_per_sentence_no_stop_words.append(nsp3)\n",
        "    nouns_count.append(my_ticket.get_nouns())\n",
        "    verbs_count.append(my_ticket.get_verbs())\n",
        "    pronoun_count.append(my_ticket.get_pronouns())\n",
        "    r1, r2, r3 = my_ticket.get_readablity()\n",
        "    flesch.append(r1)\n",
        "    coleman.append(r2)\n",
        "    gunning.append(r3)\n",
        "\n",
        "    # no opened_at \n",
        "    #d1 = str(row['opened_at'])[:10].strip()\n",
        "    #d2 = str(row['closed_at'])[:10].strip()\n",
        "    #df1, df2 = my_ticket.get_datediff(d1, d2)\n",
        "    #open_close.append(df1)\n",
        "    #open_now.append(df2)\n",
        "\n",
        "    sentiment.append(my_ticket.get_sentiment()[0])\n",
        "    sentiment_score.append(my_ticket.get_sentiment()[1])\n",
        "    try:\n",
        "        language_confidence.append(my_ticket.get_language_confidence())\n",
        "    except:\n",
        "        language_confidence.append(-1)\n",
        "\n",
        "    inquery_intensity.append(my_ticket.get_inquery_intensity())\n",
        "    special_characters.append(my_ticket.get_special_characters())\n",
        "    bl1, bl2 = my_ticket.get_bulleted_list()\n",
        "    bulleted_list.append(bl1)\n",
        "    bulleted_list_count.append(bl2)\n",
        "\n",
        "    # try:\n",
        "    #   spelling_mistakes.append(my_ticket.get_spelling_mistakes())\n",
        "    # except:\n",
        "    #   spelling_mistakes.append(0)\n",
        "    pos_tagging_max_depth.append(my_ticket.get_max_depth())\n",
        "    verb_noun_combinations.append(my_ticket.get_verb_noun_combinations()[0])\n",
        "    specific_word_count.append(my_ticket.get_specific_words_count())\n",
        "\n",
        "    # unknown_words_count.append(my_ticket.get_unknown_words_count())\n",
        "    l1, l2 = Scoring().get_links(ticket)\n",
        "    links_count.append(l2)\n",
        "    attachments_count.append(Scoring().get_attachments(ticket))\n",
        "    interactions_count.append(Scoring().get_interacts(ticket))\n",
        "\n",
        "\n",
        "    if index % 1000 == 0:\n",
        "        print(index)\n",
        "\n",
        "df['clean_problem'] = preprocessed_text\n",
        "df['clean_problem_with_time'] = preprocessed_text_time\n",
        "df['sentences_count(Problem)'] = sentences_count\n",
        "df['words_count(Problem)'] = words_count\n",
        "df['words_per_sentence(Problem)'] = words_per_sentence\n",
        "df['stop_words_count(Problem)'] = stop_words_count\n",
        "df['words_count_no_stop_words(Problem)'] = words_count_no_stop_words\n",
        "df['words_per_sentence_no_stop_words(Problem)'] = words_per_sentence_no_stop_words\n",
        "df['nouns_count(Problem)'] = nouns_count\n",
        "df['verbs_count(Problem)'] = verbs_count\n",
        "df['pronoun_count(Problem)'] = pronoun_count\n",
        "df['Flesch(Problem)'] = flesch\n",
        "df['Coleman(Problem)'] = coleman\n",
        "df['Gunning(Problem)'] = gunning\n",
        "# df['tfidf_all(Problem)'] = ticket_tfidfall\n",
        "# df['tfidf_mean(Problem)'] = ticket_tfidfmean\n",
        "df['sentiment_score(Problem)'] = sentiment_score\n",
        "df['sentiment(Problem)'] = sentiment\n",
        "df['language_confidence(Problem)'] = language_confidence\n",
        "df['inquery_intensity(Problem)'] = inquery_intensity \n",
        "df['special_characters(Problem)'] = special_characters\n",
        "df['bulleted_list(Problem)'] = bulleted_list\n",
        "df['spelling_mistakes(Problem)'] = spelling_mistakes\n",
        "df['pos_tagging_max_depth(Problem)'] = pos_tagging_max_depth\n",
        "df['verb_noun_combinations(Problem)'] = verb_noun_combinations\n",
        "df['specific_word_count(Problem)'] = specific_word_count\n",
        "# df['unknown_words_count(Problem)'] = unknown_words_count\n",
        "df['bulleted_list_count(Problem)'] = bulleted_list_count\n",
        "df['inquery_intensity(Problem)'] = inquery_intensity \n",
        "df['links_count(Problem)'] = links_count\n",
        "df['attachments_count(Problem)'] = attachments_count\n",
        "\n",
        "\n",
        "df.to_excel('output_problem_feutures_final.xlsx')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uRcoWh-pUf6s",
        "outputId": "70a9c682-e5d7-4e71-91aa-64dbe2146556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-7afc5276a93d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'special_characters(Problem)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspecial_characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bulleted_list(Problem)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbulleted_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spelling_mistakes(Problem)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspelling_mistakes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos_tagging_max_depth(Problem)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tagging_max_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verb_noun_combinations(Problem)'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverb_noun_combinations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3611\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3612\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3614\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3782\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3783\u001b[0m         \"\"\"\n\u001b[0;32m-> 3784\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3786\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4509\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \"\"\"\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values (0) does not match length of index (67171)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spelling_mistakes = []\n",
        "unknown_words_count = []\n",
        "refs = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    ticket = str(row['problem'])\n",
        "    ticket_cleantxt = str(row['clean_problem'])\n",
        "    my_ticket = Scoring(ticket_cleantxt.strip(), ticket.strip())\n",
        "    ref = my_ticket.get_refrences()\n",
        "    refs.append(ref)\n",
        "\n",
        "    try:\n",
        "      spelling_mistakes.append(my_ticket.get_spelling_mistakes())\n",
        "    except:\n",
        "      spelling_mistakes.append(0)\n",
        "\n",
        "\n",
        "    unknown_words_count.append(my_ticket.get_unknown_words_count())\n",
        "\n",
        "\n",
        "\n",
        "    if index % 1000 == 0:\n",
        "        print(index)\n",
        "\n",
        "df['refrences(Problem)'] = refs\n",
        "df['spelling_mistakes(Problem)'] = spelling_mistakes\n",
        "df['unknown_words_count(Problem)'] = unknown_words_count\n",
        "\n",
        "\n",
        "df.to_excel('/content/drive/MyDrive/output_problem_feutures_final.xlsx')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7AtKGD9yh-D",
        "outputId": "79f3e5f5-50da-454f-8c6b-1f8f401cf693"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_lem(text):\n",
        "    alph_ticket = re.sub(r'[^a-zA-Z ]', '', text)\n",
        "    lemdoc = nlp(alph_ticket)\n",
        "    lemticket = ' '.join([token.lemma_ for token in lemdoc])\n",
        "    return lemticket\n",
        "\n",
        "corpus = []\n",
        "for index, row in df.iterrows():\n",
        "    ticket = str(row['clean_problem'])\n",
        "    text = preprocess_lem(ticket)\n",
        "    corpus.append(text)\n",
        "\n",
        "ticket_tfidfall = []\n",
        "ticket_tfidfmean = []\n",
        "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(corpus)\n",
        "feature_names = tfidf_vectorizer.get_feature_names()\n",
        "for i in range(len(corpus)):\n",
        "    doc = i\n",
        "    count = 0\n",
        "    feature_index = tfidf_vectorizer_vectors[doc,:].nonzero()[1]\n",
        "    tfidf_scores = zip(feature_index, [tfidf_vectorizer_vectors[doc, x] for x in feature_index])\n",
        "    sum = 0\n",
        "    for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
        "        count += 1\n",
        "        sum += s\n",
        "    ticket_tfidfall.append(sum)\n",
        "    try:\n",
        "        ticket_tfidfmean.append(sum/count)\n",
        "    except:\n",
        "        ticket_tfidfmean.append(0)\n",
        "    if i % 1000 == 0:\n",
        "       print(i)\n",
        "\n",
        "df['tfidf_all(problem)'] = ticket_tfidfall\n",
        "df['tfidf_mean(problem)'] = ticket_tfidfmean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDEja1EY34-W",
        "outputId": "9b2e0be6-9f1a-4fd4-bcf0-46e4370e8d2b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel('/content/drive/MyDrive/output_problem_feutures_final2.xlsx')"
      ],
      "metadata": {
        "id": "veNLu_t_4A0-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text = []\n",
        "preprocessed_text_time = []\n",
        "sentences_count = []\n",
        "words_count = []\n",
        "words_per_sentence = []\n",
        "stop_words_count = []\n",
        "words_count_no_stop_words = []\n",
        "words_per_sentence_no_stop_words = []\n",
        "nouns_count = []\n",
        "verbs_count = []\n",
        "pronoun_count = []\n",
        "flesch = []\n",
        "coleman = []\n",
        "gunning = []\n",
        "sentiment_score = []\n",
        "sentiment = []\n",
        "language_confidence = []\n",
        "inquery_intensity = []\n",
        "special_characters = []\n",
        "bulleted_list = []\n",
        "bulleted_list_count = []\n",
        "verb_noun_combinations = []\n",
        "pos_tagging_max_depth = []\n",
        "specific_word_count = []\n",
        "links_count = []\n",
        "attachments_count = []\n",
        "interactions_count = []\n",
        "\n",
        "\n",
        "\n",
        "for index2, row in df.iterrows():\n",
        "    ticket = str(row['solution_sorted'])\n",
        "    ticket_cleantxt, ticket_with_time = preprocess(ticket)\n",
        "    preprocessed_text.append(ticket_cleantxt)\n",
        "    preprocessed_text_time.append(ticket_with_time)\n",
        "    my_ticket = Scoring(ticket_cleantxt.strip(), ticket.strip())\n",
        "    s1, s2 = my_ticket.get_sentences()\n",
        "    sentences_count.append(s2)\n",
        "    words_count.append(my_ticket.get_words())\n",
        "    words_per_sentence.append(my_ticket.words_per_sentences())\n",
        "    nsp1, nsp2, nsp3 = my_ticket.no_stopwords()\n",
        "    stop_words_count.append(nsp1)\n",
        "    words_count_no_stop_words.append(nsp2)\n",
        "    words_per_sentence_no_stop_words.append(nsp3)\n",
        "    nouns_count.append(my_ticket.get_nouns())\n",
        "    verbs_count.append(my_ticket.get_verbs())\n",
        "    pronoun_count.append(my_ticket.get_pronouns())\n",
        "    r1, r2, r3 = my_ticket.get_readablity()\n",
        "    flesch.append(r1)\n",
        "    coleman.append(r2)\n",
        "    gunning.append(r3)\n",
        "\n",
        "    sentiment.append(my_ticket.get_sentiment()[0])\n",
        "    sentiment_score.append(my_ticket.get_sentiment()[1])\n",
        "    try:\n",
        "        language_confidence.append(my_ticket.get_language_confidence())\n",
        "    except:\n",
        "        language_confidence.append(-1)\n",
        "\n",
        "    inquery_intensity.append(my_ticket.get_inquery_intensity())\n",
        "    special_characters.append(my_ticket.get_special_characters())\n",
        "    bl1, bl2 = my_ticket.get_bulleted_list()\n",
        "    bulleted_list.append(bl1)\n",
        "    bulleted_list_count.append(bl2)\n",
        "\n",
        "    pos_tagging_max_depth.append(my_ticket.get_max_depth())\n",
        "    verb_noun_combinations.append(my_ticket.get_verb_noun_combinations()[0])\n",
        "    specific_word_count.append(my_ticket.get_specific_words_count())\n",
        "\n",
        "    l1, l2 = Scoring().get_links(ticket)\n",
        "    links_count.append(l2)\n",
        "    attachments_count.append(Scoring().get_attachments(ticket))\n",
        "    interactions_count.append(Scoring().get_interacts(ticket))\n",
        "\n",
        "\n",
        "    if index2 % 1000 == 0:\n",
        "        print(index2)\n",
        "\n",
        "df['clean_solution'] = preprocessed_text\n",
        "df['clean_solution_with_time'] = preprocessed_text_time\n",
        "df['sentences_count(solution)'] = sentences_count\n",
        "df['words_count(solution)'] = words_count\n",
        "df['words_per_sentence(solution)'] = words_per_sentence\n",
        "df['stop_words_count(solution)'] = stop_words_count\n",
        "df['words_count_no_stop_words(solution)'] = words_count_no_stop_words\n",
        "df['words_per_sentence_no_stop_words(solution)'] = words_per_sentence_no_stop_words\n",
        "df['nouns_count(solution)'] = nouns_count\n",
        "df['verbs_count(solution)'] = verbs_count\n",
        "df['pronoun_count(solution)'] = pronoun_count\n",
        "df['Flesch(solution)'] = flesch\n",
        "df['Coleman(solution)'] = coleman\n",
        "df['Gunning(solution)'] = gunning\n",
        "df['sentiment_score(solution)'] = sentiment_score\n",
        "df['sentiment(solution)'] = sentiment\n",
        "df['language_confidence(solution)'] = language_confidence\n",
        "df['inquery_intensity(solution)'] = inquery_intensity \n",
        "df['special_characters(solution)'] = special_characters\n",
        "df['bulleted_list(solution)'] = bulleted_list\n",
        "df['pos_tagging_max_depth(solution)'] = pos_tagging_max_depth\n",
        "df['verb_noun_combinations(solution)'] = verb_noun_combinations\n",
        "df['specific_word_count(solution)'] = specific_word_count\n",
        "df['bulleted_list_count(solution)'] = bulleted_list_count\n",
        "df['inquery_intensity(solution)'] = inquery_intensity \n",
        "df['links_count(solution)'] = links_count\n",
        "df['attachments_count(solution)'] = attachments_count\n",
        "\n",
        "\n",
        "df.to_excel('/content/drive/MyDrive/output_solution_feutures_final.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTzbZKd-_hjp",
        "outputId": "0053226c-c77c-40ff-8961-c59f49aa0dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/drive/MyDrive/output_solution_feutures_final.xlsx', sheet_name=0)\n",
        "print(\"Succesfully read excel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdiNo3C200ez",
        "outputId": "6cd9368a-5cf6-4599-ac3a-104310defb02"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Succesfully read excel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spelling_mistakes = []\n",
        "unknown_words_count = []\n",
        "refs = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    ticket = str(row['solution_sorted'])\n",
        "    ticket_cleantxt = str(row['clean_solution'])\n",
        "    my_ticket = Scoring(ticket_cleantxt.strip(), ticket.strip())\n",
        "    ref = my_ticket.get_refrences()\n",
        "    refs.append(ref)\n",
        "\n",
        "    try:\n",
        "      spelling_mistakes.append(my_ticket.get_spelling_mistakes())\n",
        "    except:\n",
        "      spelling_mistakes.append(0)\n",
        "\n",
        "\n",
        "    unknown_words_count.append(my_ticket.get_unknown_words_count())\n",
        "\n",
        "\n",
        "\n",
        "    if index % 1000 == 0:\n",
        "        print(index)\n",
        "\n",
        "df['refrences(Solution)'] = refs\n",
        "df['spelling_mistakes(Solution)'] = spelling_mistakes\n",
        "df['unknown_words_count(Solution)'] = unknown_words_count\n",
        "\n",
        "\n",
        "df.to_excel('/content/drive/MyDrive/output_solution_feutures_final2.xlsx')"
      ],
      "metadata": {
        "id": "2_WTwVlLChTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d87383-9804-471d-d171-2ebb9e4b8b15"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_lem(text):\n",
        "    alph_ticket = re.sub(r'[^a-zA-Z ]', '', text)\n",
        "    lemdoc = nlp(alph_ticket)\n",
        "    lemticket = ' '.join([token.lemma_ for token in lemdoc])\n",
        "    return lemticket\n",
        "\n",
        "corpus = []\n",
        "for index, row in df.iterrows():\n",
        "    ticket = str(row['clean_solution'])\n",
        "    text = preprocess_lem(ticket)\n",
        "    corpus.append(text)\n",
        "\n",
        "ticket_tfidfall = []\n",
        "ticket_tfidfmean = []\n",
        "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(corpus)\n",
        "feature_names = tfidf_vectorizer.get_feature_names()\n",
        "for i in range(len(corpus)):\n",
        "    doc = i\n",
        "    count = 0\n",
        "    feature_index = tfidf_vectorizer_vectors[doc,:].nonzero()[1]\n",
        "    tfidf_scores = zip(feature_index, [tfidf_vectorizer_vectors[doc, x] for x in feature_index])\n",
        "    sum = 0\n",
        "    for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
        "        count += 1\n",
        "        sum += s\n",
        "    ticket_tfidfall.append(sum)\n",
        "    try:\n",
        "        ticket_tfidfmean.append(sum/count)\n",
        "    except:\n",
        "        ticket_tfidfmean.append(0)\n",
        "    if i % 1000 == 0:\n",
        "       print(i)\n",
        "\n",
        "df['tfidf_all(solution)'] = ticket_tfidfall\n",
        "df['tfidf_mean(solution)'] = ticket_tfidfmean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_eXZphz2JH3",
        "outputId": "d58309d2-7df9-4acb-8b6b-ba601d6ad06d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "49000\n",
            "50000\n",
            "51000\n",
            "52000\n",
            "53000\n",
            "54000\n",
            "55000\n",
            "56000\n",
            "57000\n",
            "58000\n",
            "59000\n",
            "60000\n",
            "61000\n",
            "62000\n",
            "63000\n",
            "64000\n",
            "65000\n",
            "66000\n",
            "67000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel('/content/drive/MyDrive/output_solution_feutures_final2.xlsx')"
      ],
      "metadata": {
        "id": "VSjwUvlW3gQn"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}